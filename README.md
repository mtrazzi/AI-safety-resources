# AI-safety-resources
A collection of resources to get into AI Safety.

## Getting Started

### Books

[Superintelligence(2014)](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) by Nick Bostrom

[Life 3.0(2017)](https://www.amazon.com/Life-3-0-Being-Artificial-Intelligence/dp/1101946598) by Max Tegmark

### Blogs

[The wait but why series on the AI Revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)

### Youtube Channels

[Robert Miles hilarious and insightful channel about AI Safety](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg)

## Learn Technical Skills

### AI Safety Study Group

[Road to AI Safety Excellence](http://aisafety.camp/about/)

[Paris AI Safety Study Group](https://www.meetup.com/Paris-AI-Safety/)

### Learn Reinforcement learning

[Jupyter notebooks of RL material](https://github.com/dennybritz/reinforcement-learning)

[Reinforcement learning environments from deepmind](https://github.com/deepmind/ai-safety-gridworlds)

### Inverse Reinforcement Learning

[Algorithms for Inverse RL (Andrew Ng, 2000)](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf)

### Corrigibility

[Corrigibility (Soares, Fallenstein, Yudkowsky & Armstrong)](https://intelligence.org/files/Corrigibility.pdf)

[The Off-Switch game](https://arxiv.org/pdf/1611.08219.pdf)

## Further Reading

### Publications

[Machine Intelligence Research Institute](https://intelligence.org/research/)

[Future of Humanity Institute](https://www.fhi.ox.ac.uk/research/research-areas/)

[Future of Life](https://futureoflife.org/ai-safety-research/)

### Collection of collection of resources

[How to get into AI Safety in 80,000 hours](https://80000hours.org/ai-safety-syllabus/)

[Great syllabus from a researcher at Deepmind working on AI Safety](https://vkrakovna.wordpress.com/ai-safety-resources/)

[Effective Altruism AI Safety Literature](http://effective-altruism.com/ea/1iu/2018_ai_safety_literature_review_and_charity/)

### Newsletters

[Alignment Newsletter](https://www.lesswrong.com/posts/rhdzxSfLXpxZRsHg6/alignment-newsletter-13-07-02-18)


